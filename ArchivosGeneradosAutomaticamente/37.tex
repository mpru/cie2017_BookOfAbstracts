\A
{REGRESIÓN EN GRANDES DIMENSIONES. ESTUDIO COMPARATIVO DE LA REGRESIÓN LASSO BAYESIANA Y UNA ALTERNATIVA LASSO BAYESIANA ROBUSTA}
{\Presenting{MARÍA BELÉN ALLASIA}$^{1,2}$\index{ALLASIA, M}, MARCIA D'ELIA BRANCO$^3$\index{BRANCO, M} y MARTA BEATRIZ QUAGLINO$^1$\index{QUAGLINO, M}}
{\Afilliation{$^1$IITAE. FACULTAD DE CIENCIAS ECONÓMICAS Y ESTADÍSTICA. UNIVERSIDAD NACIONAL DE ROSARIO}
\Afilliation{$^2$CONICET}
\Afilliation{$^3$INSTITUTO DE MATEMÁTICA E ESTATÍSTICA, UNIVERSIDADE DE SÃO PAULO}
\\\Email{mallasia@fcecon.unr.edu.ar}}
{data mining; regresión penalizada; regresión robusta; selección de variables} 
 {Otras aplicaciones} 
 {Métodos bayesianos} 
 {37} 
 {88-1}
{En el análisis estadístico de grandes volúmenes de datos es indispensable identificar aquellas variables que provean información valiosa. Al ajustar modelos de regresión, el método tradicional de estimación brinda estimadores insesgados pero poco precisos al trabajar con gran número de predictores, por lo que se proponen métodos de regularización para mejorar estos ajustes. Estos métodos, también conocidos como penalizados, consisten en ajustar el modelo eliminando predictores en el proceso de estimación, forzando que los coeficientes pequeños sean igual a cero. Entre éstos, la regresión LASSO (Least Absolute Shrinkage and Selection Operator) se ha convertido en una alternativa ampliamente utilizada. Sin embargo, la aplicación de técnicas clásicas requiere el cumplimiento de supuestos que rara vez son verificados en la práctica, por lo que varios autores han trabajado en el desarrollo de estimadores robustos, probando que éstos tienen un desempeño similar a los clásicos cuando se cumplen los supuestos. En el presente trabajo se estudia la metodología LASSO implementada mediante el enfoque estadístico Bayesiano, para la estimación de los parámetros de regresión en el contexto de muchas variables predictoras. Se propone una versión de Regresión LASSO Bayesiana Robusta considerando en el modelo jerárquico que los errores aleatorios siguen una distribución a priori t-Student, lo que se consigue asumiendo que los errores aleatorios siguen una distribución normal con mezcla de escala, cuya variancia tiene una distribución hiperpriori Gamma; en lugar del supuesto tradicional de distribución normal. Se simularon diferentes escenarios con mayor número de parámetros que observaciones, cambiando el número de parámetros significativos a estimar. Se comparó el desempeño de ambos métodos a través del Error Cuadrático Medio (ECM). Siguiendo el principio propuesto por Hastie, Tibshirani y Friedman (2009) de “apostar al esparcimiento” -que propone elegir aquel procedimiento que funcione bien en problemas de datos esparcidos, ya que ningún método funciona bien en problemas densos-, se generaron escenarios donde el número de parámetros significativos fuera pequeño en relación al total de variables predictoras. Al incrementar el número de parámetros significativos, el ECM de cada técnica aumenta, siendo estrictamente menor el ECM de la Regresión LASSO Bayesiana Robusta y con menor variabilidad en todos los casos. Además, al asumir un modelo no normal para las observaciones, la regresión penalizada robusta otorga mejores estimaciones, siendo más precisos los estimadores calculados. En conclusión, se recomienda usar estimaciones robustas de los parámetros de regresión ya que proveen un ajuste más apropiado del comportamiento de los datos en cualquier contexto, sin demandar el cumplimiento de supuestos acerca del comportamiento de los mismos.}
