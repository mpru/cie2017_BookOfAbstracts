\A
{ESTUDIO DEL COMPORTAMIENTO DE LOS ESTIMADORES DE REGRESIÓN EN GRANDES DIMENSIONES DE DATOS}
{\Presenting{IVÁN MILLANES}\index{MILLANES, I}, MARÍA BELÉN ALLASIA\index{ALLASIA, M} y MARTA BEATRIZ QUAGLINO\index{QUAGLINO, M}}
{\Afilliation{INSTITUTO DE INVESTIGACIONES TEÓRICAS Y APLICADAS DE LA ESCUELA DE ESTADÍSTICA (IITAE). FACULTAD DE CIENCIAS ECONÓMICAS Y ESTADÍSTICA, UNR.}
\\\Email{ivanmillanes26@gmail.com}}
{mco; ridge; lasso; big data; ecm} 
 {Otras aplicaciones} 
 {Modelos de regresión} 
 {57} 
 {133-1}
{Para estimar los parámetros de un modelo de regresión lineal, es común utilizar el método de mínimos cuadrados ordinarios (MCO), método popular por su fácil aplicación y buenas propiedades. Sin embargo, cuando el número de variables explicativas (p) es mayor que el número de observaciones (n) el estimador MCO no es único y pierde sentido la interpretación de los resultados. Incluso cuando p es menor pero cercano a n -pese a la existencia de una estimación mínimo-cuadrática única- puede no ser conveniente la utilización de este método debido a la gran variabilidad de las estimaciones. En este contexto es común utilizar métodos de regularización, que minimizan una Suma de Cuadrados del Error Penalizada: regresiones Ridge y LASSO En el presente trabajo se efectuó un estudio por simulación para comparar el desempeño de los métodos de regresión en escenarios donde p es igual a n modificando el número de parámetros significativos en el modelo (k), esto es la cantidad de parámetros distintos de cero. A partir de la generación de conjuntos de observaciones de la variable respuesta (Y), se calculó el estimador MCO y se ajustaron los caminos de solución Ridge y LASSO para 50 valores del parámetro de penalización, reteniendo en cada caso sólo el estimador del vector de parámetros que produce el menor Error Cuadrático Medio (ECM) Se construyeron las distribuciones empíricas de los estimadores obtenidos con cada método considerando globalmente aquellos que son significativos y los que no lo son. Se observó que para estimar parámetros no significativos tanto los estimadores Ridge como LASSO resultaron insesgados, siendo el método LASSO más preciso. Al estimar parámetros significativos y para k pequeño, ambas técnicas proveen resultados sesgados, siendo el desempeño de Ridge peor que el de LASSO en este sentido. Al aumentar k, el método LASSO fuerza a muchos estimadores a ser cero a pesar de que sean significativos, mientras que Ridge mantiene prácticamente el mismo comportamiento que para k pequeño. Por otra parte, MCO provee estimadores insesgados para todos los parámetros pero con variabilidad mucho mayor que las regresiones penalizadas. Al comparar los ECM promedios, se observó que la regresión LASSO tiene mejor desempeño que Ridge cuando k es pequeño. A medida que aumenta k, las regresiones penalizadas empeoran su performance, funcionando mejor Ridge que LASSO. En general el ECM promedio del método MCO fue significativamente mayor, por lo cual MCO no es una alternativa admisible en estos escenarios}
